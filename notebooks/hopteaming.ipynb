{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = '../data/hop_teaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 153332 rows\n",
      "Added 155155 rows\n",
      "Added 157854 rows\n",
      "Added 157310 rows\n",
      "Added 153192 rows\n",
      "Added 150092 rows\n",
      "Added 150721 rows\n",
      "Added 155427 rows\n",
      "Added 157028 rows\n",
      "Added 150601 rows\n",
      "Added 151017 rows\n",
      "Added 159637 rows\n",
      "Added 160506 rows\n",
      "Added 150137 rows\n",
      "Added 149767 rows\n",
      "Added 163595 rows\n",
      "Added 162169 rows\n",
      "Added 163097 rows\n",
      "Added 159830 rows\n",
      "Added 153770 rows\n",
      "Added 161355 rows\n",
      "Added 160586 rows\n",
      "Added 160145 rows\n",
      "Added 158177 rows\n",
      "Added 155225 rows\n",
      "Added 156739 rows\n",
      "Added 158768 rows\n",
      "Added 159394 rows\n",
      "Added 161332 rows\n",
      "Added 157262 rows\n",
      "Added 153534 rows\n",
      "Added 162608 rows\n",
      "Added 159421 rows\n",
      "Added 155861 rows\n",
      "Added 157002 rows\n",
      "Added 157531 rows\n",
      "Added 155274 rows\n",
      "Added 156349 rows\n",
      "Added 156270 rows\n",
      "Added 155120 rows\n",
      "Added 154790 rows\n",
      "Added 158463 rows\n",
      "Added 157091 rows\n",
      "Added 157828 rows\n",
      "Added 156131 rows\n",
      "Added 153782 rows\n",
      "Added 152566 rows\n",
      "Added 153639 rows\n",
      "Added 152816 rows\n",
      "Added 155932 rows\n",
      "Added 160923 rows\n",
      "Added 159131 rows\n",
      "Added 153004 rows\n",
      "Added 155318 rows\n",
      "Added 157505 rows\n",
      "Added 157891 rows\n",
      "Added 160236 rows\n",
      "Added 155207 rows\n",
      "Added 152751 rows\n",
      "Added 152921 rows\n",
      "Added 154099 rows\n",
      "Added 155929 rows\n",
      "Added 157068 rows\n",
      "Added 154373 rows\n",
      "Added 155322 rows\n",
      "Added 156928 rows\n",
      "Added 159010 rows\n",
      "Added 162739 rows\n",
      "Added 163854 rows\n",
      "Added 153311 rows\n",
      "Added 155330 rows\n",
      "Added 158812 rows\n",
      "Added 160392 rows\n",
      "Added 158895 rows\n",
      "Added 154972 rows\n",
      "Added 152543 rows\n",
      "Added 151585 rows\n",
      "Added 151417 rows\n",
      "Added 152525 rows\n",
      "Added 153412 rows\n",
      "Added 153767 rows\n",
      "Added 155236 rows\n",
      "Added 156896 rows\n",
      "Added 158909 rows\n",
      "Added 159901 rows\n",
      "Added 152410 rows\n",
      "Added 150956 rows\n",
      "Added 154371 rows\n",
      "Added 158771 rows\n",
      "Added 152039 rows\n",
      "Added 153309 rows\n",
      "Added 156550 rows\n",
      "Added 158148 rows\n",
      "Added 153513 rows\n",
      "Added 153830 rows\n",
      "Added 156629 rows\n",
      "Added 159764 rows\n",
      "Added 156759 rows\n",
      "Added 156886 rows\n",
      "Added 158805 rows\n",
      "Added 157590 rows\n",
      "Added 159468 rows\n",
      "Added 158334 rows\n",
      "Added 159817 rows\n",
      "Added 159517 rows\n",
      "Added 163549 rows\n",
      "Added 161669 rows\n",
      "Added 158406 rows\n",
      "Added 157315 rows\n",
      "Added 158427 rows\n",
      "Added 157184 rows\n",
      "Added 157375 rows\n",
      "Added 158418 rows\n",
      "Added 157796 rows\n",
      "Added 156584 rows\n",
      "Added 156028 rows\n",
      "Added 155650 rows\n",
      "Added 160337 rows\n",
      "Added 160416 rows\n",
      "Added 155615 rows\n",
      "Added 150528 rows\n",
      "Added 150372 rows\n",
      "Added 158038 rows\n",
      "Added 159939 rows\n",
      "Added 157261 rows\n",
      "Added 158125 rows\n",
      "Added 155720 rows\n",
      "Added 154208 rows\n",
      "Added 154692 rows\n",
      "Added 151142 rows\n",
      "Added 157783 rows\n",
      "Added 155573 rows\n",
      "Added 153847 rows\n",
      "Added 154309 rows\n",
      "Added 159579 rows\n",
      "Added 159023 rows\n",
      "Added 157854 rows\n",
      "Added 154474 rows\n",
      "Added 154061 rows\n",
      "Added 158204 rows\n",
      "Added 160511 rows\n",
      "Added 161180 rows\n",
      "Added 158601 rows\n",
      "Added 158161 rows\n",
      "Added 165975 rows\n",
      "Added 168285 rows\n",
      "Added 158173 rows\n",
      "Added 156550 rows\n",
      "Added 157654 rows\n",
      "Added 164835 rows\n",
      "Added 154467 rows\n",
      "Added 150065 rows\n",
      "Added 155202 rows\n",
      "Added 154861 rows\n",
      "Added 153221 rows\n",
      "Added 153348 rows\n",
      "Added 157281 rows\n",
      "Added 159952 rows\n",
      "Added 158005 rows\n",
      "Added 158373 rows\n",
      "Added 155599 rows\n",
      "Added 156498 rows\n",
      "Added 160168 rows\n",
      "Added 157808 rows\n",
      "Added 153804 rows\n",
      "Added 155157 rows\n",
      "Added 158505 rows\n",
      "Added 159821 rows\n",
      "Added 159862 rows\n",
      "Added 158121 rows\n",
      "Added 157017 rows\n",
      "Added 156837 rows\n",
      "Added 155004 rows\n",
      "Added 153835 rows\n",
      "Added 154513 rows\n",
      "Added 159593 rows\n",
      "Added 159008 rows\n",
      "Added 159687 rows\n",
      "Added 161332 rows\n",
      "Added 156628 rows\n",
      "Added 156077 rows\n",
      "Added 150047 rows\n",
      "Added 152979 rows\n",
      "Added 159947 rows\n",
      "Added 158120 rows\n",
      "Added 157479 rows\n",
      "Added 157625 rows\n",
      "Added 156722 rows\n",
      "Added 156493 rows\n",
      "Added 153332 rows\n",
      "Added 153252 rows\n",
      "Added 154761 rows\n",
      "Added 156254 rows\n",
      "Added 161743 rows\n",
      "Added 159139 rows\n",
      "Added 156362 rows\n",
      "Added 157131 rows\n",
      "Added 160939 rows\n",
      "Added 153648 rows\n",
      "Added 148247 rows\n",
      "Added 150870 rows\n",
      "Added 150577 rows\n",
      "Added 158882 rows\n",
      "Added 157773 rows\n",
      "Added 155283 rows\n",
      "Added 152518 rows\n",
      "Added 174085 rows\n",
      "Added 201843 rows\n",
      "Added 153951 rows\n",
      "Added 153059 rows\n",
      "Added 149259 rows\n",
      "Added 150759 rows\n",
      "Added 152751 rows\n",
      "Added 156751 rows\n",
      "Added 158300 rows\n",
      "Added 157067 rows\n",
      "Added 157527 rows\n",
      "Added 137582 rows\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1000000\n",
    "n = 1\n",
    "\n",
    "# The following code reads the CSV file in \"chunks\", processes each chunk and adds to the database\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    for chunk in pd.read_csv('../data/DocGraph_Hop_Teaming_2018_Commercial/DocGraph_Hop_Teaming_2018.csv', chunksize=chunksize):\n",
    "        chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]  # Clean up the column names\n",
    "        chunk = chunk[(chunk.transaction_count >=50) & (chunk.average_day_wait <50)]\n",
    "        chunk.to_sql('hopteaming', db, if_exists='append', index=False)  # Append the chunk to a hops table\n",
    "        n += chunk.shape[0]\n",
    "        print(\"Added \" + str(chunk.shape[0]) + \" rows. Total = \"+ str(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the taxonomy code for a given row\n",
    "def get_taxo_code(row):\n",
    "    code_prefix = 'Healthcare Provider Taxonomy Code_'\n",
    "    switch_prefix = 'Healthcare Provider Primary Taxonomy Switch_'\n",
    "    for num in range(1,16):\n",
    "        if row[switch_prefix+str(num)] == 'Y':\n",
    "            break\n",
    "    return row[code_prefix+str(num)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 914032 rows. Total = 914032\n",
      "Added 903772 rows. Total = 1817804\n",
      "Added 945915 rows. Total = 2763719\n",
      "Added 968799 rows. Total = 3732518\n",
      "Added 979097 rows. Total = 4711615\n",
      "Added 987765 rows. Total = 5699380\n",
      "Added 991930 rows. Total = 6691310\n",
      "Added 995928 rows. Total = 7687238\n",
      "Added 180369 rows. Total = 7867607\n"
     ]
    }
   ],
   "source": [
    "# This section creates the npps table\n",
    "\n",
    "chunksize = 1000000\n",
    "n = 0\n",
    "\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    for chunk in pd.read_csv('../data/NPPES_Data_Dissemination_February_2024/nppes_main.csv', dtype=str, chunksize=chunksize):\n",
    "        taxonomy_cols = [col for col in chunk.columns if 'Taxonomy Code' in col or 'Taxonomy Switch' in col] #gets the column names which have \"Taxonomy Code\" or \"Taxonomy Switch\" in them\n",
    "\n",
    "        chunk = chunk[['NPI', 'Entity Type Code', 'Provider Organization Name (Legal Business Name)', 'Provider Last Name (Legal Name)',\n",
    "                        'Provider First Name', 'Provider Middle Name', 'Provider Name Prefix Text', 'Provider Name Suffix Text',\n",
    "                        'Provider Credential Text', 'Provider First Line Business Practice Location Address', 'Provider Second Line Business Practice Location Address',\n",
    "                        'Provider Business Practice Location Address City Name', 'Provider Business Practice Location Address State Name',\n",
    "                        'Provider Business Practice Location Address Postal Code'] + taxonomy_cols] # selects the columns we want\n",
    "\n",
    "        rename_dict = {'Provider Organization Name (Legal Business Name)': 'Provider Business Name', \n",
    "                    'Provider Last Name (Legal Name)': 'Provider Last Name',\n",
    "                    'Provider Business Practice Location Address Postal Code': 'Provider Zip Code'\n",
    "                    }\n",
    "        df = chunk.copy()\n",
    "        df = df.rename(rename_dict, axis=1)\n",
    "        df[\"taxo_code\"] = chunk.apply(get_taxo_code, axis=1)\n",
    "        df = df.drop(columns = taxonomy_cols)\n",
    "        df =df[df.NPI.notnull() & df.taxo_code.notnull() & df['Provider Zip Code'].notnull()]\n",
    "        df['Provider Zip Code'] = df['Provider Zip Code'].apply(lambda x: str(x)[:5] if pd.notnull(x) else x)\n",
    "        df.columns = [x.lower().replace(' ', '_').replace(\"(\",\"\").replace(\")\",\"\") for x in df.columns] \n",
    "        df.to_sql('npi', db, if_exists='append', index=False)  # Append the chunk to an npps table\n",
    "        n += df.shape[0]\n",
    "        print(\"Added \" + str(df.shape[0]) + \" rows. Total = \"+ str(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section creates the code_to_classification table\n",
    "taxonomy_code_to_classification= pd.read_csv('../data/nucc_taxonomy_240.csv', dtype=str)\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    taxonomy_code_to_classification.to_sql('taxonomy', db, if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 rows. Total = 0\n",
      "Added 157 rows. Total = 157\n",
      "Added 0 rows. Total = 157\n",
      "Added 0 rows. Total = 157\n",
      "Added 0 rows. Total = 157\n"
     ]
    }
   ],
   "source": [
    "# This section creates the zip_to_cbsa table\n",
    "chunksize = 10000\n",
    "n = 0\n",
    "\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    for df in pd.read_csv('../data/ZIP_CBSA_122023.csv', dtype=str, chunksize=chunksize):\n",
    "        df = df.drop(columns=['RES_RATIO','BUS_RATIO', 'OTH_RATIO', 'TOT_RATIO'])\n",
    "        df = df[df.ZIP.notnull() & df.CBSA.notnull() & (df.CBSA == '34980')]\n",
    "        df.to_sql('zip_cbsa', db, if_exists='append', index=False)  # Append the chunk to the zip_to_cbsa table\n",
    "        n += df.shape[0]\n",
    "        print(\"Added \" + str(df.shape[0]) + \" rows. Total = \"+ str(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH \n",
    "\n",
    "# hopteaming = pd.read_csv('../data/DocGraph_Hop_Teaming_2018_Commercial/DocGraph_Hop_Teaming_2018.csv', \n",
    "#                           nrows = 100)\n",
    "\n",
    "\n",
    "# chunk = pd.read_csv('../data/NPPES_Data_Dissemination_February_2024/nppes_main.csv', dtype=str,\n",
    "#                           nrows = 100)\n",
    "# taxonomy_cols = [col for col in chunk.columns if 'Taxonomy Code' in col or 'Taxonomy Switch' in col]\n",
    "\n",
    "# chunk = chunk[['NPI', 'Entity Type Code', 'Provider Organization Name (Legal Business Name)', 'Provider Last Name (Legal Name)', \n",
    "#              'Provider First Name', 'Provider Middle Name', 'Provider Name Prefix Text', 'Provider Name Suffix Text', \n",
    "#              'Provider Credential Text', 'Provider First Line Business Practice Location Address', 'Provider Second Line Business Practice Location Address',\n",
    "#              'Provider Business Practice Location Address City Name', 'Provider Business Practice Location Address State Name', \n",
    "#              'Provider Business Practice Location Address Postal Code']+taxonomy_cols]\n",
    "# df = chunk.copy()\n",
    "# df[\"taxo_code\"] = chunk.apply(get_taxo_code, axis=1)\n",
    "# df = df.drop(columns = taxonomy_cols)\n",
    "# df =df[df.NPI.notnull() & df.taxo_code.notnull()]\n",
    "# df.columns = [x.lower().replace(' ', '_') for x in df.columns] \n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_csv('../data/DocGraph_Hop_Teaming_2018_Commercial/DocGraph_Hop_Teaming_2018.csv', nrows=100)\n",
    "# # df = df[df.ZIP.notnull() & df.CBSA.notnull()]\n",
    "# df[(df.transaction_count >=50) & (df.average_day_wait <50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(database_path)\n",
    "\n",
    "db.execute('CREATE INDEX IF NOT EXISTS idx_npi ON npi(npi)')\n",
    "# db.execute('CREATE INDEX IF NOT EXISTS idx_from_npi ON hopteaming(from_npi)')\n",
    "# db.execute('CREATE INDEX IF NOT EXISTS idx_to_npi ON hopteaming(to_npi)')\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   null_count\n",
       "0           0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT COUNT(*) AS null_count\n",
    "FROM npi\n",
    "WHERE provider_zip_code IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "with sqlite3.connect(database_path) as db: \n",
    "    res = pd.read_sql(query, db)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(database_path) as db:\n",
    "    # Drop the table\n",
    "    db.execute('DROP TABLE IF EXISTS zip_cbsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =\"\"\"\n",
    "SELECT \n",
    "    ht.from_npi as referrer_npi, \n",
    "    ht.to_npi as hosp_npi, \n",
    "    ht.transaction_count AS txns,\n",
    "    ht.patient_count AS patients,\n",
    "    npi.provider_business_name AS hosp_business_name, \n",
    "    tx.Grouping AS hosp_grouping, \n",
    "    tx.Classification as hosp_classification,\n",
    "    npi.provider_zip_code as hosp_zip_code,\n",
    "    npi.provider_business_practice_location_address_city_name as hosp_city,\n",
    "    npi.provider_business_practice_location_address_state_name as hosp_state\n",
    "FROM \n",
    "    hopteaming AS ht\n",
    "    INNER JOIN npi ON ht.to_npi = npi.npi\n",
    "    INNER JOIN zip_cbsa ON npi.provider_zip_code = zip_cbsa.ZIP\n",
    "    INNER JOIN taxonomy AS tx ON npi.taxo_code = tx.Code\n",
    "WHERE \n",
    "    npi.entity_type_code = '2'\n",
    "\"\"\"\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    referrals = pd.read_sql(query, db)\n",
    "    db.execute('DROP TABLE IF EXISTS nash_referrals')\n",
    "    referrals.to_sql('nash_referrals', db, if_exists='append', index=False)\n",
    "    db.execute('CREATE INDEX IF NOT EXISTS idx_referrer_npi ON nash_referrals(referrer_npi)')\n",
    "    db.execute('CREATE INDEX IF NOT EXISTS idx_hosp_npi ON nash_referrals(hosp_npi)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
