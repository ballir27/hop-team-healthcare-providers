{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Code to delete all tables in the database\n",
    "# # # ONLY RUN THIS TO RESET THE ENTIRE DATABASE\n",
    "\n",
    "# database_path = '../data/hop_teaming'\n",
    "\n",
    "# # Open the database connection\n",
    "# with sqlite3.connect(database_path) as db:\n",
    "#     # Create a cursor object to execute SQL commands\n",
    "#     cursor = db.cursor()\n",
    "\n",
    "#     # Query the names of all tables in the database\n",
    "#     cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    \n",
    "#     # Fetch all table names\n",
    "#     table_names = cursor.fetchall()\n",
    "#     print(table_names)\n",
    "\n",
    "#     # Iterate through each table and drop it\n",
    "#     for table in table_names:\n",
    "#         cursor.execute(f\"DROP TABLE IF EXISTS {table[0]}\")\n",
    "#     db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = '../data/hop_teaming'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1000000 rows\n",
      "Added 2000000 rows\n",
      "Added 3000000 rows\n",
      "Added 4000000 rows\n",
      "Added 5000000 rows\n",
      "Added 6000000 rows\n",
      "Added 7000000 rows\n",
      "Added 8000000 rows\n",
      "Added 9000000 rows\n",
      "Added 10000000 rows\n",
      "Added 11000000 rows\n",
      "Added 12000000 rows\n",
      "Added 13000000 rows\n",
      "Added 14000000 rows\n",
      "Added 15000000 rows\n",
      "Added 16000000 rows\n",
      "Added 17000000 rows\n",
      "Added 18000000 rows\n",
      "Added 19000000 rows\n",
      "Added 20000000 rows\n",
      "Added 21000000 rows\n",
      "Added 22000000 rows\n",
      "Added 23000000 rows\n",
      "Added 24000000 rows\n",
      "Added 25000000 rows\n",
      "Added 26000000 rows\n",
      "Added 27000000 rows\n",
      "Added 28000000 rows\n",
      "Added 29000000 rows\n",
      "Added 30000000 rows\n",
      "Added 31000000 rows\n",
      "Added 32000000 rows\n",
      "Added 33000000 rows\n",
      "Added 34000000 rows\n",
      "Added 35000000 rows\n",
      "Added 36000000 rows\n",
      "Added 37000000 rows\n",
      "Added 38000000 rows\n",
      "Added 39000000 rows\n",
      "Added 40000000 rows\n",
      "Added 41000000 rows\n",
      "Added 42000000 rows\n",
      "Added 43000000 rows\n",
      "Added 44000000 rows\n",
      "Added 45000000 rows\n",
      "Added 46000000 rows\n",
      "Added 47000000 rows\n",
      "Added 48000000 rows\n",
      "Added 49000000 rows\n",
      "Added 50000000 rows\n",
      "Added 51000000 rows\n",
      "Added 52000000 rows\n",
      "Added 53000000 rows\n",
      "Added 54000000 rows\n",
      "Added 55000000 rows\n",
      "Added 56000000 rows\n",
      "Added 57000000 rows\n",
      "Added 58000000 rows\n",
      "Added 59000000 rows\n",
      "Added 60000000 rows\n",
      "Added 61000000 rows\n",
      "Added 62000000 rows\n",
      "Added 63000000 rows\n",
      "Added 64000000 rows\n",
      "Added 65000000 rows\n",
      "Added 66000000 rows\n",
      "Added 67000000 rows\n",
      "Added 68000000 rows\n",
      "Added 69000000 rows\n",
      "Added 70000000 rows\n",
      "Added 71000000 rows\n",
      "Added 72000000 rows\n",
      "Added 73000000 rows\n",
      "Added 74000000 rows\n",
      "Added 75000000 rows\n",
      "Added 76000000 rows\n",
      "Added 77000000 rows\n",
      "Added 78000000 rows\n",
      "Added 79000000 rows\n",
      "Added 80000000 rows\n",
      "Added 81000000 rows\n",
      "Added 82000000 rows\n",
      "Added 83000000 rows\n",
      "Added 84000000 rows\n",
      "Added 85000000 rows\n",
      "Added 86000000 rows\n",
      "Added 87000000 rows\n",
      "Added 88000000 rows\n",
      "Added 89000000 rows\n",
      "Added 90000000 rows\n",
      "Added 91000000 rows\n",
      "Added 92000000 rows\n",
      "Added 93000000 rows\n",
      "Added 94000000 rows\n",
      "Added 95000000 rows\n",
      "Added 96000000 rows\n",
      "Added 97000000 rows\n",
      "Added 98000000 rows\n",
      "Added 99000000 rows\n",
      "Added 100000000 rows\n",
      "Added 101000000 rows\n",
      "Added 102000000 rows\n",
      "Added 103000000 rows\n",
      "Added 104000000 rows\n",
      "Added 105000000 rows\n",
      "Added 106000000 rows\n",
      "Added 107000000 rows\n",
      "Added 108000000 rows\n",
      "Added 109000000 rows\n",
      "Added 110000000 rows\n",
      "Added 111000000 rows\n",
      "Added 112000000 rows\n",
      "Added 113000000 rows\n",
      "Added 114000000 rows\n",
      "Added 115000000 rows\n",
      "Added 116000000 rows\n",
      "Added 117000000 rows\n",
      "Added 118000000 rows\n",
      "Added 119000000 rows\n",
      "Added 120000000 rows\n",
      "Added 121000000 rows\n",
      "Added 122000000 rows\n",
      "Added 123000000 rows\n",
      "Added 124000000 rows\n",
      "Added 125000000 rows\n",
      "Added 126000000 rows\n",
      "Added 127000000 rows\n",
      "Added 128000000 rows\n",
      "Added 129000000 rows\n",
      "Added 130000000 rows\n",
      "Added 131000000 rows\n",
      "Added 132000000 rows\n",
      "Added 133000000 rows\n",
      "Added 134000000 rows\n",
      "Added 135000000 rows\n",
      "Added 136000000 rows\n",
      "Added 137000000 rows\n",
      "Added 138000000 rows\n",
      "Added 139000000 rows\n",
      "Added 140000000 rows\n",
      "Added 141000000 rows\n",
      "Added 142000000 rows\n",
      "Added 143000000 rows\n",
      "Added 144000000 rows\n",
      "Added 145000000 rows\n",
      "Added 146000000 rows\n",
      "Added 147000000 rows\n",
      "Added 148000000 rows\n",
      "Added 149000000 rows\n",
      "Added 150000000 rows\n",
      "Added 151000000 rows\n",
      "Added 152000000 rows\n",
      "Added 153000000 rows\n",
      "Added 154000000 rows\n",
      "Added 155000000 rows\n",
      "Added 156000000 rows\n",
      "Added 157000000 rows\n",
      "Added 158000000 rows\n",
      "Added 159000000 rows\n",
      "Added 160000000 rows\n",
      "Added 161000000 rows\n",
      "Added 162000000 rows\n",
      "Added 163000000 rows\n",
      "Added 164000000 rows\n",
      "Added 165000000 rows\n",
      "Added 166000000 rows\n",
      "Added 167000000 rows\n",
      "Added 168000000 rows\n",
      "Added 169000000 rows\n",
      "Added 170000000 rows\n",
      "Added 171000000 rows\n",
      "Added 172000000 rows\n",
      "Added 173000000 rows\n",
      "Added 174000000 rows\n",
      "Added 175000000 rows\n",
      "Added 176000000 rows\n",
      "Added 177000000 rows\n",
      "Added 178000000 rows\n",
      "Added 179000000 rows\n",
      "Added 180000000 rows\n",
      "Added 181000000 rows\n",
      "Added 182000000 rows\n",
      "Added 183000000 rows\n",
      "Added 184000000 rows\n",
      "Added 185000000 rows\n",
      "Added 186000000 rows\n",
      "Added 187000000 rows\n",
      "Added 188000000 rows\n",
      "Added 189000000 rows\n",
      "Added 190000000 rows\n",
      "Added 191000000 rows\n",
      "Added 192000000 rows\n",
      "Added 193000000 rows\n",
      "Added 194000000 rows\n",
      "Added 195000000 rows\n",
      "Added 196000000 rows\n",
      "Added 197000000 rows\n",
      "Added 198000000 rows\n",
      "Added 199000000 rows\n",
      "Added 200000000 rows\n",
      "Added 201000000 rows\n",
      "Added 202000000 rows\n",
      "Added 203000000 rows\n",
      "Added 204000000 rows\n",
      "Added 205000000 rows\n",
      "Added 206000000 rows\n",
      "Added 207000000 rows\n",
      "Added 208000000 rows\n",
      "Added 209000000 rows\n",
      "Added 210000000 rows\n",
      "Added 211000000 rows\n",
      "Added 212000000 rows\n",
      "Added 213000000 rows\n",
      "Added 214000000 rows\n",
      "Added 215000000 rows\n",
      "Added 216000000 rows\n",
      "Added 217000000 rows\n",
      "Added 218000000 rows\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1000000\n",
    "n = 1\n",
    "\n",
    "# The following code reads the CSV file in \"chunks\", processes each chunk and adds to the database\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    for chunk in pd.read_csv('../data/DocGraph_Hop_Teaming_2018_Commercial/DocGraph_Hop_Teaming_2018.csv', chunksize=chunksize):\n",
    "        chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]  # Clean up the column names\n",
    "        chunk.to_sql('hopteaming', db, if_exists='append', index=False)  # Append the chunk to a hops table\n",
    "        print(\"Added \" + str(n * chunksize) + \" rows\")\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the taxonomy code for a given row\n",
    "def get_taxo_code(row):\n",
    "    code_prefix = 'Healthcare Provider Taxonomy Code_'\n",
    "    switch_prefix = 'Healthcare Provider Primary Taxonomy Switch_'\n",
    "    for num in range(1,16):\n",
    "        if row[switch_prefix+str(num)] == 'Y':\n",
    "            break\n",
    "    return row[code_prefix+str(num)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1000000 rows\n",
      "Added 2000000 rows\n",
      "Added 3000000 rows\n",
      "Added 4000000 rows\n",
      "Added 5000000 rows\n",
      "Added 6000000 rows\n",
      "Added 7000000 rows\n",
      "Added 8000000 rows\n",
      "Added 9000000 rows\n"
     ]
    }
   ],
   "source": [
    "# This section creates the npps table\n",
    "\n",
    "chunksize = 1000000\n",
    "n = 1\n",
    "\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    for chunk in pd.read_csv('../data/NPPES_Data_Dissemination_February_2024/nppes_main.csv', dtype=str, chunksize=chunksize):\n",
    "        taxonomy_cols = [col for col in chunk.columns if 'Taxonomy Code' in col or 'Taxonomy Switch' in col] #gets the column names which have \"Taxonomy Code\" or \"Taxonomy Switch\" in them\n",
    "\n",
    "        chunk = chunk[['NPI', 'Entity Type Code', 'Provider Organization Name (Legal Business Name)', 'Provider Last Name (Legal Name)',\n",
    "                        'Provider First Name', 'Provider Middle Name', 'Provider Name Prefix Text', 'Provider Name Suffix Text',\n",
    "                        'Provider Credential Text', 'Provider First Line Business Practice Location Address', 'Provider Second Line Business Practice Location Address',\n",
    "                        'Provider Business Practice Location Address City Name', 'Provider Business Practice Location Address State Name',\n",
    "                        'Provider Business Practice Location Address Postal Code'] + taxonomy_cols] # selects the columns we want\n",
    "\n",
    "        df = chunk.copy() # making a copy of the chunk to avoid \"SettingWithCopyWarning\" wwarnings\n",
    "        df[\"taxo_code\"] = chunk.apply(get_taxo_code, axis=1) # making a column called taxo_code which will hold the taxonomy code for that row\n",
    "        df = df.drop(columns=taxonomy_cols) # dropping the taxonomy columns as we don't need them anymore\n",
    "        df = df[df.NPI.notnull() & df.taxo_code.notnull()] # drop the rows in which NPI an taxonomy code are null (don't see how these would be useful)\n",
    "        df.columns = [x.lower().replace(' ', '_') for x in df.columns]  # Clean up the column names\n",
    "        df.to_sql('npi', db, if_exists='append', index=False)  # Append the chunk to an npps table\n",
    "        print(\"Added \" + str(n * chunksize) + \" rows\")\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section creates the code_to_classification table\n",
    "taxonomy_code_to_classification= pd.read_csv('../data/nucc_taxonomy_240.csv', dtype=str,\n",
    "                          nrows = 100)\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    taxonomy_code_to_classification.to_sql('taxonomy', db, if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 10000 rows\n",
      "Added 10000 rows\n",
      "Added 10000 rows\n",
      "Added 10000 rows\n",
      "Added 7598 rows\n"
     ]
    }
   ],
   "source": [
    "# This section creates the zip_to_cbsa table\n",
    "chunksize = 10000\n",
    "n = 0\n",
    "\n",
    "with sqlite3.connect(database_path) as db:\n",
    "    for df in pd.read_csv('../data/ZIP_CBSA_122023.csv', dtype=str, chunksize=chunksize):\n",
    "        df = df.drop(columns=['RES_RATIO','BUS_RATIO', 'OTH_RATIO', 'TOT_RATIO'])\n",
    "        df = df[df.ZIP.notnull() & df.CBSA.notnull()]\n",
    "        df.to_sql('zip_cbsa', db, if_exists='append', index=False)  # Append the chunk to the zip_to_cbsa table\n",
    "        print(\"Added \" + str(df.shape[0]) + \" rows\")\n",
    "        n += df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH \n",
    "\n",
    "# hopteaming = pd.read_csv('../data/DocGraph_Hop_Teaming_2018_Commercial/DocGraph_Hop_Teaming_2018.csv', \n",
    "#                           nrows = 100)\n",
    "\n",
    "\n",
    "# chunk = pd.read_csv('../data/NPPES_Data_Dissemination_February_2024/nppes_main.csv', dtype=str,\n",
    "#                           nrows = 100)\n",
    "# taxonomy_cols = [col for col in chunk.columns if 'Taxonomy Code' in col or 'Taxonomy Switch' in col]\n",
    "\n",
    "# chunk = chunk[['NPI', 'Entity Type Code', 'Provider Organization Name (Legal Business Name)', 'Provider Last Name (Legal Name)', \n",
    "#              'Provider First Name', 'Provider Middle Name', 'Provider Name Prefix Text', 'Provider Name Suffix Text', \n",
    "#              'Provider Credential Text', 'Provider First Line Business Practice Location Address', 'Provider Second Line Business Practice Location Address',\n",
    "#              'Provider Business Practice Location Address City Name', 'Provider Business Practice Location Address State Name', \n",
    "#              'Provider Business Practice Location Address Postal Code']+taxonomy_cols]\n",
    "# df = chunk.copy()\n",
    "# df[\"taxo_code\"] = chunk.apply(get_taxo_code, axis=1)\n",
    "# df = df.drop(columns = taxonomy_cols)\n",
    "# df =df[df.NPI.notnull() & df.taxo_code.notnull()]\n",
    "# df.columns = [x.lower().replace(' ', '_') for x in df.columns] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
