{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c6d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e695ed",
   "metadata": {},
   "source": [
    "**Dataset Downloads**\n",
    "\n",
    "- Hop Teaming data can be found at https://careset.com/docgraph-hop-teaming-dataset/.\n",
    "- Download the NPPES Data Dissemination from https://download.cms.gov/nppes/NPI_Files.html.\n",
    "- Download the taxonomy code to classification crosswalk from the National Uniform Claim Committee (https://www.nucc.org/index.php/code-sets-mainmenu-41/provider-taxonomy-mainmenu-40/csv-mainmenu-57).\n",
    "- Download the Zip code to CBSA crosswalk from here: https://www.huduser.gov/portal/datasets/usps_crosswalk.html. \n",
    "\n",
    "**Goal: Create a SQL database for the four datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0df2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../data/hop_teaming.sqlite'\n",
    "\n",
    "chunksize = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb496d0",
   "metadata": {},
   "source": [
    "**#1 hopteaming dataset**\n",
    "\n",
    "**Remove \"accidental\" referrals in hopteaming DataFrame.** \n",
    "\n",
    "Filter for records where transaction_count is at least 50 and the average_day_wait is less than 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40413f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 rows added\n",
      "2000000 rows added\n",
      "3000000 rows added\n",
      "4000000 rows added\n",
      "5000000 rows added\n",
      "6000000 rows added\n",
      "7000000 rows added\n",
      "8000000 rows added\n",
      "9000000 rows added\n",
      "10000000 rows added\n",
      "11000000 rows added\n",
      "12000000 rows added\n",
      "13000000 rows added\n",
      "14000000 rows added\n",
      "15000000 rows added\n",
      "16000000 rows added\n",
      "17000000 rows added\n",
      "18000000 rows added\n",
      "19000000 rows added\n",
      "20000000 rows added\n",
      "21000000 rows added\n",
      "22000000 rows added\n",
      "23000000 rows added\n",
      "24000000 rows added\n",
      "25000000 rows added\n",
      "26000000 rows added\n",
      "27000000 rows added\n",
      "28000000 rows added\n",
      "29000000 rows added\n",
      "30000000 rows added\n",
      "31000000 rows added\n",
      "32000000 rows added\n",
      "33000000 rows added\n",
      "34000000 rows added\n",
      "35000000 rows added\n",
      "36000000 rows added\n",
      "37000000 rows added\n",
      "38000000 rows added\n",
      "39000000 rows added\n",
      "40000000 rows added\n",
      "41000000 rows added\n",
      "42000000 rows added\n",
      "43000000 rows added\n",
      "44000000 rows added\n",
      "45000000 rows added\n",
      "46000000 rows added\n",
      "47000000 rows added\n",
      "48000000 rows added\n",
      "49000000 rows added\n",
      "50000000 rows added\n",
      "51000000 rows added\n",
      "52000000 rows added\n",
      "53000000 rows added\n",
      "54000000 rows added\n",
      "55000000 rows added\n",
      "56000000 rows added\n",
      "57000000 rows added\n",
      "58000000 rows added\n",
      "59000000 rows added\n",
      "60000000 rows added\n",
      "61000000 rows added\n",
      "62000000 rows added\n",
      "63000000 rows added\n",
      "64000000 rows added\n",
      "65000000 rows added\n",
      "66000000 rows added\n",
      "67000000 rows added\n",
      "68000000 rows added\n",
      "69000000 rows added\n",
      "70000000 rows added\n",
      "71000000 rows added\n",
      "72000000 rows added\n",
      "73000000 rows added\n",
      "74000000 rows added\n",
      "75000000 rows added\n",
      "76000000 rows added\n",
      "77000000 rows added\n",
      "78000000 rows added\n",
      "79000000 rows added\n",
      "80000000 rows added\n",
      "81000000 rows added\n",
      "82000000 rows added\n",
      "83000000 rows added\n",
      "84000000 rows added\n",
      "85000000 rows added\n",
      "86000000 rows added\n",
      "87000000 rows added\n",
      "88000000 rows added\n",
      "89000000 rows added\n",
      "90000000 rows added\n",
      "91000000 rows added\n",
      "92000000 rows added\n",
      "93000000 rows added\n",
      "94000000 rows added\n",
      "95000000 rows added\n",
      "96000000 rows added\n",
      "97000000 rows added\n",
      "98000000 rows added\n",
      "99000000 rows added\n",
      "100000000 rows added\n",
      "101000000 rows added\n",
      "102000000 rows added\n",
      "103000000 rows added\n",
      "104000000 rows added\n",
      "105000000 rows added\n",
      "106000000 rows added\n",
      "107000000 rows added\n",
      "108000000 rows added\n",
      "109000000 rows added\n",
      "110000000 rows added\n",
      "111000000 rows added\n",
      "112000000 rows added\n",
      "113000000 rows added\n",
      "114000000 rows added\n",
      "115000000 rows added\n",
      "116000000 rows added\n",
      "117000000 rows added\n",
      "118000000 rows added\n",
      "119000000 rows added\n",
      "120000000 rows added\n",
      "121000000 rows added\n",
      "122000000 rows added\n",
      "123000000 rows added\n",
      "124000000 rows added\n",
      "125000000 rows added\n",
      "126000000 rows added\n",
      "127000000 rows added\n",
      "128000000 rows added\n",
      "129000000 rows added\n",
      "130000000 rows added\n",
      "131000000 rows added\n",
      "132000000 rows added\n",
      "133000000 rows added\n",
      "134000000 rows added\n",
      "135000000 rows added\n",
      "136000000 rows added\n",
      "137000000 rows added\n",
      "138000000 rows added\n",
      "139000000 rows added\n",
      "140000000 rows added\n",
      "141000000 rows added\n",
      "142000000 rows added\n",
      "143000000 rows added\n",
      "144000000 rows added\n",
      "145000000 rows added\n",
      "146000000 rows added\n",
      "147000000 rows added\n",
      "148000000 rows added\n",
      "149000000 rows added\n",
      "150000000 rows added\n",
      "151000000 rows added\n",
      "152000000 rows added\n",
      "153000000 rows added\n",
      "154000000 rows added\n",
      "155000000 rows added\n",
      "156000000 rows added\n",
      "157000000 rows added\n",
      "158000000 rows added\n",
      "159000000 rows added\n",
      "160000000 rows added\n",
      "161000000 rows added\n",
      "162000000 rows added\n",
      "163000000 rows added\n",
      "164000000 rows added\n",
      "165000000 rows added\n",
      "166000000 rows added\n",
      "167000000 rows added\n",
      "168000000 rows added\n",
      "169000000 rows added\n",
      "170000000 rows added\n",
      "171000000 rows added\n",
      "172000000 rows added\n",
      "173000000 rows added\n",
      "174000000 rows added\n",
      "175000000 rows added\n",
      "176000000 rows added\n",
      "177000000 rows added\n",
      "178000000 rows added\n",
      "179000000 rows added\n",
      "180000000 rows added\n",
      "181000000 rows added\n",
      "182000000 rows added\n",
      "183000000 rows added\n",
      "184000000 rows added\n",
      "185000000 rows added\n",
      "186000000 rows added\n",
      "187000000 rows added\n",
      "188000000 rows added\n",
      "189000000 rows added\n",
      "190000000 rows added\n",
      "191000000 rows added\n",
      "192000000 rows added\n",
      "193000000 rows added\n",
      "194000000 rows added\n",
      "195000000 rows added\n",
      "196000000 rows added\n",
      "197000000 rows added\n",
      "198000000 rows added\n",
      "199000000 rows added\n",
      "200000000 rows added\n",
      "201000000 rows added\n",
      "202000000 rows added\n",
      "203000000 rows added\n",
      "204000000 rows added\n",
      "205000000 rows added\n",
      "206000000 rows added\n",
      "207000000 rows added\n",
      "208000000 rows added\n",
      "209000000 rows added\n",
      "210000000 rows added\n",
      "211000000 rows added\n",
      "212000000 rows added\n",
      "213000000 rows added\n",
      "214000000 rows added\n",
      "215000000 rows added\n",
      "216000000 rows added\n",
      "217000000 rows added\n",
      "218000000 rows added\n",
      "Done\n",
      "Total rows: 34184634\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "total_rows = 0\n",
    "\n",
    "with sqlite3.connect(db_path) as db:\n",
    "    for chunk in pd.read_csv('../data/DocGraph_Hop_Teaming_2018.csv', chunksize = chunksize):                \n",
    "        chunk = chunk[(chunk['transaction_count'] >= 50) & (chunk['average_day_wait'] <= 50)]\n",
    "        chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]\n",
    "        chunk.to_sql('hopteaming', db, if_exists = 'append', index = False) \n",
    "        print(str(n * chunksize) + \" rows added\")\n",
    "        n += 1\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "    db.execute('CREATE INDEX from_npi ON hopteaming(from_npi)')\n",
    "    db.execute('CREATE INDEX to_npi ON hopteaming(to_npi)')\n",
    "\n",
    "print(\"Done\")\n",
    "print(\"Total rows:\", total_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b8b91",
   "metadata": {},
   "source": [
    "**#2 Taxonomy Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e07750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 rows added\n",
      "Done\n",
      "Total rows: 874\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "total_rows = 0\n",
    "\n",
    "with sqlite3.connect(db_path) as db:\n",
    "    for chunk in pd.read_csv('../data/nucc_taxonomy_240.csv', chunksize = chunksize): \n",
    "        chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]\n",
    "        chunk.to_sql('taxonomy', db, if_exists = 'append', index = False) \n",
    "        print(str(n * chunksize) + \" rows added\")\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "print(\"Done\")\n",
    "print(\"Total rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d23f9",
   "metadata": {},
   "source": [
    "**#3 Zip_cbsa Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7d1002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 rows added\n",
      "Done\n",
      "Total rows: 47598\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "total_rows = 0\n",
    "\n",
    "with sqlite3.connect(db_path) as db:\n",
    "    for chunk in pd.read_csv('../data/ZIP_CBSA_122023.csv', chunksize = chunksize): \n",
    "        chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]\n",
    "        chunk.to_sql('zip_cbsa', db, if_exists = 'append', index = False, dtype={'zip': str}) \n",
    "        print(str(n * chunksize) + \" rows added\")\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "print(\"Done\")\n",
    "print(\"Total rows:\", total_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9e5bd",
   "metadata": {},
   "source": [
    "**#4 NPPES (NPI) Dataset**\n",
    "\n",
    "**Select the relevant columns in NPPES dataset for this project.**\n",
    "\n",
    "- 'NPI'\n",
    "- Entity Type, indicated by the 'Entity Type Code' field:\n",
    "    - 1 = Provider (doctors, nurses, etc.)\n",
    "    - 2 = Facility (Hospitals, Urgent Care, Doctors Offices)\n",
    "- Entity Name: Either First/Last or Organization or Other Organization Name contained in the following fields:\n",
    "    - 'Provider Organization Name (Legal Business Name)'\n",
    "    - 'Provider Last Name (Legal Name)'\n",
    "    - 'Provider First Name'\n",
    "    - 'Provider Middle Name'\n",
    "    - 'Provider Name Prefix Text'\n",
    "    - 'Provider Name Suffix Text'\n",
    "    - 'Provider Credential Text'\n",
    "- Address: Business Practice Location (not mailing), contained in the following fields:\n",
    "    - 'Provider First Line Business Practice Location Address'\n",
    "    - 'Provider Second Line Business Practice Location Address'\n",
    "    - 'Provider Business Practice Location Address City Name'\n",
    "    - 'Provider Business Practice Location Address State Name'\n",
    "    - 'Provider Business Practice Location Address Postal Code'\n",
    "- The provider's taxonomy code, which is contained in one of the 'Healthcare Provider Taxonomy Code*' columns.\n",
    "\n",
    "**Retrieve the provider's taxonomy code from the npidata DataFrame.**\n",
    "\n",
    "A provider can have up to 15 taxonomy codes, but we want the one which has Primary Switch = Y in the associated 'Healthcare Provider Primary Taxonomy Switch*' field. Note that this does not always occur in spot 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2b727c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 rows added\n",
      "2000000 rows added\n",
      "3000000 rows added\n",
      "4000000 rows added\n",
      "5000000 rows added\n",
      "6000000 rows added\n",
      "7000000 rows added\n",
      "8000000 rows added\n",
      "9000000 rows added\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "there is already a table named npi",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m         n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     44\u001b[0m         total_rows \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m---> 46\u001b[0m     db\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCREATE INDEX npi ON npi(npi)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m     db\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCREATE INDEX provider_organization_name_(legal_business_name) ON npi(provider_organization_name_(legal_business_name))\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOperationalError\u001b[0m: there is already a table named npi"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "total_rows = 0\n",
    "\n",
    "with sqlite3.connect(db_path) as db:\n",
    "    for chunk in pd.read_csv('../data/npidata_pfile_20050523-20240211.csv', \n",
    "                             low_memory = False, \n",
    "                             chunksize = chunksize): \n",
    "        # Create Taxonomy Code column\n",
    "        chunk['Healthcare Provider Taxonomy Code'] = None\n",
    "        \n",
    "        # Fill the value in the column if Switch Code column has value Y\n",
    "        for i in range(1, 16):  # For the 15 columns relating to taxonomy code\n",
    "            code_col = f'Healthcare Provider Taxonomy Code_{i}'\n",
    "            switch_col = f'Healthcare Provider Primary Taxonomy Switch_{i}'\n",
    "            if switch_col in chunk.columns:\n",
    "                chunk.loc[chunk[switch_col] == 'Y', \n",
    "                                   'Healthcare Provider Taxonomy Code'] = chunk.loc[chunk[switch_col] == 'Y', \n",
    "                                                                                             code_col]\n",
    "        # Create a zip code column \n",
    "        chunk['Zip'] = pd.to_numeric(chunk['Provider Business Practice Location Address Postal Code'].str[:5], errors='coerce')\n",
    "        \n",
    "        # Select the specific columns\n",
    "        chunk = chunk[['NPI',\n",
    "                          'Entity Type Code',\n",
    "                          'Provider Organization Name (Legal Business Name)',\n",
    "                          'Provider Last Name (Legal Name)',\n",
    "                          'Provider First Name',\n",
    "                          'Provider Middle Name',\n",
    "                          'Provider Name Prefix Text',\n",
    "                          'Provider Name Suffix Text',\n",
    "                          'Provider Credential Text',\n",
    "                          'Provider First Line Business Practice Location Address',\n",
    "                          'Provider Second Line Business Practice Location Address',\n",
    "                          'Provider Business Practice Location Address City Name',\n",
    "                          'Provider Business Practice Location Address Postal Code',\n",
    "                          'Healthcare Provider Taxonomy Code',\n",
    "                          'Zip'\n",
    "                         ]]\n",
    "        chunk.columns = [x.lower().replace(' ', '_') for x in chunk.columns]\n",
    "\n",
    "        chunk.to_sql('npi', db, if_exists = 'append', index = False) \n",
    "        print(str(n * chunksize) + \" rows added\")\n",
    "        n += 1\n",
    "        total_rows += len(chunk)\n",
    "        \n",
    "\n",
    "print(\"Done\")\n",
    "print(\"Total rows:\", total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f8fcd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect('../data/hop_teaming.sqlite')\n",
    "db.execute('CREATE INDEX IF NOT EXISTS npi_index ON npi(npi)')\n",
    "db.execute('CREATE INDEX IF NOT EXISTS provider_organization_name_index ON npi(\"provider_organization_name_(legal_business_name)\")')\n",
    "db.execute('CREATE INDEX IF NOT EXISTS zip_index ON npi(Zip)')\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
